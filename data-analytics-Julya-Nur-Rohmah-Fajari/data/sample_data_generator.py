# -*- coding: utf-8 -*-
"""sample_data_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HOeTVCoU0twrcgG0NTGuPE3Q80QoYxtt
"""

#import library
import pandas as pd
import ast
import numpy as np
import os

"""#Import File"""

#import file dari github
url = "https://raw.githubusercontent.com/cinte-id/book-app/main/backend/books.json"
df = pd.read_json(url)

#tampilkan file
pd.set_option('display.max_columns', None)
print(df.head())

#ubah ke file csv
df.to_csv('books_raw.csv', index=False)

df_csv = pd.read_csv('books_raw.csv')

#ubah string JSON di kolom 'books' jadi dictionary Python
df_csv['books'] = df_csv['books'].apply(ast.literal_eval)

#pecah kolom 'books' jadi kolom terpisah (normalize)
df_clean = pd.json_normalize(df_csv['books'])

#tampilkan hasilnya
df_clean.head()

"""#Cleaning"""

#pengecekan format data
df_clean.info()

"""Tidak ada data kosong jadi lanjut pada tahap pengecekan duplikat"""

#mengecek apakah ada duplikat
duplicates = df_clean.duplicated()
print(duplicates.value_counts())  # Berapa True/False

#menampilkan baris yang duplikat
print(df_clean[duplicates])

"""Tidak terdapat duplikat pada data ini dan dapat dilanjutkan pada tahap create sample user interaction data

#Create Sample User Interaction Data
"""

#buat daftar sample 20 user fiktif
user_ids = [f'U{i:03d}' for i in range(1, 21)]

#buat data interaksi 100
n_records = 100
df_interactions = pd.DataFrame({
    'user_id': np.random.choice(user_ids, n_records),
    'book_id': np.random.choice(df_clean['id'], n_records),
    'event_type': np.random.choice(['view', 'search', 'rating'], n_records, p=[0.4, 0.4, 0.2]),
    'rating': np.random.randint(1, 6, n_records)  # 1–5
})

#hapus rating untuk event selain 'rating'
df_interactions.loc[df_interactions['event_type'] != 'rating', 'rating'] = np.nan

#tambahkan timestamp acak (Januari–Oktober 2025)
df_interactions['timestamp'] = pd.to_datetime(
    np.random.choice(pd.date_range('2025-01-01', '2025-10-22', freq='h'), n_records)
)

#tampilkan data
df_interactions.head()

#cek info df_intercation
df_interactions.info()

#cek persebaran event_type
df_interactions['event_type'].value_counts(normalize=True).round(2)

"""diketahui bahwa persebaran event type rating sebanyak 20 sehingga data rating hanya 20"""

#cek nilai rating hanya muncul di event_type='rating'
invalid_ratings = df_interactions[(df_interactions['event_type'] != 'rating') & (df_interactions['rating'].notna())]
print(f"Jumlah rating tidak valid: {len(invalid_ratings)} (harusnya 0)\n")

#cek duplikasi
duplicates = df_interactions.duplicated(subset=['user_id', 'book_id', 'event_type']).sum()
print(f"Jumlah duplikasi user+book+event: {duplicates}\n")

#cek baris duplikasi
df_interactions[df_interactions.duplicated(subset=['user_id', 'book_id', 'event_type'], keep=False)].sort_values(['user_id', 'book_id'])

#cek variasi user dan buku
print(f"Total unique user: {df_interactions['user_id'].nunique()}")
print(f"Total unique book: {df_interactions['book_id'].nunique()}")
print(f"Rentang tanggal interaksi: {df_interactions['timestamp'].min()} - {df_interactions['timestamp'].max()}")

"""Duplikasi pada data ini tidak perlu dihapus karena data tidak menghasiolkan redundant dan dapat digunakan untuk menganalisis pola perilaku user dan nilai uniq pada data ini tetap 20 user dan 8 buku.

#Generating Reading Progress Data
"""

#buat list user_id sesuai data interaksi
user_ids = [f"U{str(i).zfill(3)}" for i in range(1, 21)]
book_ids = [1, 2, 3, 4, 5, 6, 7, 8]

#generate data acak reading progress
progress_data = []
for user in user_ids:
    for book in np.random.choice(book_ids, size=np.random.randint(1, 4), replace=False):
        total_pages = np.random.randint(150, 500)
        pages_read = np.random.randint(0, total_pages)
        completion_rate = round((pages_read / total_pages) * 100, 2)
        if completion_rate == 0:
            status = "Not Started"
        elif completion_rate < 100:
            status = "In Progress"
        else:
            status = "Completed"

        progress_data.append({
            "user_id": user,
            "book_id": book,
            "pages_read": pages_read,
            "total_pages": total_pages,
            "completion_rate": completion_rate,
            "status": status
        })

df_progress = pd.DataFrame(progress_data)
df_progress.head()

"""#Data Transformation"""

print("Calculate Devide Matriks")
#pastikan df_progress terdapat kolom pages_read, total_pages, dan waktu baca
df_progress['completion_rate'] =( (df_progress['pages_read'] / df_progress['total_pages']) * 100).round(2)

#tambahkan kolom baca acak per jam
df_progress['hours_spent'] = np.random.uniform(1, 10, len(df_progress)).round(2)

#hitung kecepatan membaca halaman per jam
df_progress['reading_speed_numeric'] = (df_progress['pages_read'] / df_progress['hours_spent']).round(2)
df_progress['reading_speed'] = df_progress['reading_speed_numeric'].astype(str) + ' halaman/jam'

df_progress.head()

print("Create User Segments Based on Reading Behavior")
#hitung rata-rata completion rate & speed per user
user_summary = df_progress.groupby('user_id').agg({
    'completion_rate': 'mean',
    'reading_speed_numeric': 'mean'
}).reset_index()

#bulatkan nilai jadi 2 angka di belakang koma
user_summary['completion_rate'] = user_summary['completion_rate'].round(2)
user_summary['reading_speed_numeric'] = user_summary['reading_speed_numeric'].round(2)

#buat segmen pembaca
bins = [0, 33, 66, 100]
labels = ['Casual Reader', 'Regular Reader', 'Avid Reader']
user_summary['reader_segment'] = pd.cut(user_summary['completion_rate'], bins=bins, labels=labels)

user_summary.head()

print("Agregar Data for Dashboard Consumption")
print("Rata-Rata per Buku")
book_summary = df_progress.groupby('book_id').agg({
    'completion_rate': 'mean',
    'reading_speed_numeric': 'mean' # Use the numeric column for mean aggregation
}).reset_index()

book_summary['completion_rate'] = book_summary['completion_rate'].round(2)
book_summary['reading_speed_numeric'] = book_summary['reading_speed_numeric'].round(2)

display(book_summary)

print("Agregar Data for Dashboard Consumption")
print("Rata-Rata per Segmen")
segment_summary = user_summary.groupby('reader_segment', observed=False).agg({
    'completion_rate': 'mean',
    'reading_speed_numeric': 'mean' # Use the correct column name
}).reset_index()

segment_summary['completion_rate'] = segment_summary['completion_rate'].round(2)
segment_summary['reading_speed_numeric'] = segment_summary['reading_speed_numeric'].round(2)

display(segment_summary)

"""#Simpan File"""

#Simpan semua hasil transformasi ke file csv
df_clean.to_csv('clean_books.csv', index=False)
df_interactions.to_csv('user_interactions.csv', index=False)
df_progress.to_csv('reading_progress.csv', index=False)
user_summary.to_csv('user_summary.csv', index=False)
book_summary.to_csv('book_summary.csv', index=False)
segment_summary.to_csv('segment_summary.csv', index=False)

print("Semua file CSV berhasil disimpan di folder kerja saat ini!")

#Simpan file ke drive
from google.colab import drive
import os

drive.mount('/content/drive')

#ganti path dengan nama folder drive
save_path = '/content/drive/MyDrive/data analytic engineer/dataclean/'

#buat direktori folder
os.makedirs(save_path, exist_ok=True)

df_clean.to_csv(save_path +'clean_books.csv', index=False)
df_interactions.to_csv(save_path + 'user_interactions.csv', index=False)
df_progress.to_csv(save_path + 'reading_progress.csv', index=False)
user_summary.to_csv(save_path + 'user_summary.csv', index=False)
book_summary.to_csv(save_path + 'book_summary.csv', index=False)
segment_summary.to_csv(save_path + 'segment_summary.csv', index=False)

print("Semua file CSV berhasil disimpan di Google Drive!")